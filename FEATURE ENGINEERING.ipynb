{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c01747a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "[1/6] –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ preprocessing...\n",
      " –°–æ–∑–¥–∞–Ω–∏–µ book_tags_clean...\n",
      "  –ó–∞–≥—Ä—É–∂–µ–Ω–æ:\n",
      "     ‚Ä¢ Train: 874,496 –æ—Ü–µ–Ω–æ–∫\n",
      "     ‚Ä¢ Test: 107,260 –æ—Ü–µ–Ω–æ–∫\n",
      "     ‚Ä¢ Book embeddings: 10,000 –∫–Ω–∏–≥ √ó 768\n",
      "     ‚Ä¢ User embeddings (train): 53,424 √ó 768\n",
      "     ‚Ä¢ User embeddings (test): 35,659 √ó 768\n",
      "     ‚Ä¢ Book tags: 10,000 –∫–Ω–∏–≥\n",
      "     ‚Ä¢ Holdout users: 35,659\n",
      "\n",
      "================================================================================\n",
      "[2/6] –°–û–ó–î–ê–ù–ò–ï USER FEATURES\n",
      "================================================================================\n",
      "\n",
      "  –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...\n",
      "     avg_user_rating, ratings_count, unique_books_count\n",
      "\n",
      "  –í—ã—á–∏—Å–ª–µ–Ω–∏–µ tag_vocab_size...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     Processing users: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53424/53424 [00:04<00:00, 11887.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tag_vocab_size\n",
      "\n",
      "  –í—ã—á–∏—Å–ª–µ–Ω–∏–µ activity_score...\n",
      "      activity_score\n",
      "\n",
      "  –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...\n",
      "\n",
      "   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º:\n",
      "     ‚Ä¢ new            : 25,571 ( 47.9%)\n",
      "     ‚Ä¢ active         : 16,240 ( 30.4%)\n",
      "     ‚Ä¢ inactive       :  8,343 ( 15.6%)\n",
      "     ‚Ä¢ very_active    :  3,270 (  6.1%)\n",
      "\n",
      "   User features —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: user_features.csv\n",
      "     –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: 5 (–∫—Ä–æ–º–µ user_id)\n",
      "\n",
      "================================================================================\n",
      "[3/6] –°–û–ó–î–ê–ù–ò–ï BOOK FEATURES\n",
      "================================================================================\n",
      "\n",
      "  –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –∫–Ω–∏–≥...\n",
      "      book_avg_rating, book_ratings_count\n",
      "\n",
      "  –í—ã—á–∏—Å–ª–µ–Ω–∏–µ book_popularity...\n",
      "     book_popularity\n",
      "\n",
      "  Book features —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: book_features.csv\n",
      "     –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: 3 (–∫—Ä–æ–º–µ book_id)\n",
      "\n",
      "================================================================================\n",
      "[4/6] –°–û–ó–î–ê–ù–ò–ï INTERACTION FEATURES\n",
      "================================================================================\n",
      "\n",
      " –≠—Ç–æ –∑–∞–π–º–µ—Ç –≤—Ä–µ–º—è (–º–Ω–æ–≥–æ –ø–∞—Ä user-book)...\n",
      "  –ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è: 874,496 train + 107,260 test\n",
      "\n",
      "  –°–æ–∑–¥–∞–Ω–∏–µ interaction features –¥–ª—è train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 874496/874496 [27:40<00:00, 526.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train interaction features: 874,496\n",
      "\n",
      "  –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ user tags –¥–ª—è test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     User tags: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53424/53424 [00:06<00:00, 7790.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  User tags –≥–æ—Ç–æ–≤—ã: 53,424 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
      "\n",
      "  –°–æ–∑–¥–∞–Ω–∏–µ interaction features –¥–ª—è test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     Test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 107260/107260 [00:09<00:00, 10794.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Test interaction features: 107,260\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# FEATURE ENGINEERING \n",
    "# ================================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ================================================================================\n",
    "# 1. –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n[1/6] –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ preprocessing...\")\n",
    "\n",
    "# –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤\n",
    "train_df = pd.read_csv('train_dataset.csv')\n",
    "test_df = pd.read_csv('test_dataset.csv')\n",
    "complete_df = pd.read_csv('complete_dataset.csv')\n",
    "\n",
    "with open('holdout_dict.pkl', 'rb') as f:\n",
    "    holdout_dict = pickle.load(f)\n",
    "\n",
    "#  —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ embeddings\n",
    "with open('book_embeddings.pkl', 'rb') as f:\n",
    "    book_embeddings_dict = pickle.load(f)\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (768-dim)\n",
    "    book_embeddings = book_embeddings_dict['book_combined']\n",
    "\n",
    "with open('user_embeddings.pkl', 'rb') as f:\n",
    "    user_embeddings_dict = pickle.load(f)\n",
    "    user_embeddings_train = user_embeddings_dict['train']\n",
    "    user_embeddings_test = user_embeddings_dict['test']\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º book_tags_clean –∏–∑ complete_dataset\n",
    "print(\" –°–æ–∑–¥–∞–Ω–∏–µ book_tags_clean...\")\n",
    "book_tags_clean = {}\n",
    "for _, row in complete_df[['book_id', 'tags_text']].drop_duplicates('book_id').iterrows():\n",
    "    if pd.notna(row['tags_text']) and row['tags_text'].strip() != '':\n",
    "        book_tags_clean[row['book_id']] = row['tags_text'].split()\n",
    "    else:\n",
    "        book_tags_clean[row['book_id']] = []\n",
    "\n",
    "print(f\"  –ó–∞–≥—Ä—É–∂–µ–Ω–æ:\")\n",
    "print(f\"     ‚Ä¢ Train: {len(train_df):,} –æ—Ü–µ–Ω–æ–∫\")\n",
    "print(f\"     ‚Ä¢ Test: {len(test_df):,} –æ—Ü–µ–Ω–æ–∫\")\n",
    "print(f\"     ‚Ä¢ Book embeddings: {len(book_embeddings):,} –∫–Ω–∏–≥ √ó 768\")\n",
    "print(f\"     ‚Ä¢ User embeddings (train): {len(user_embeddings_train):,} √ó 768\")\n",
    "print(f\"     ‚Ä¢ User embeddings (test): {len(user_embeddings_test):,} √ó 768\")\n",
    "print(f\"     ‚Ä¢ Book tags: {len(book_tags_clean):,} –∫–Ω–∏–≥\")\n",
    "print(f\"     ‚Ä¢ Holdout users: {len(holdout_dict):,}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 2. USER FEATURES (4 –ø—Ä–∏–∑–Ω–∞–∫–∞)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2/6] –°–û–ó–î–ê–ù–ò–ï USER FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n  –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...\")\n",
    "\n",
    "# 2.1. –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "user_stats = train_df.groupby('user_id').agg({\n",
    "    'rating': ['mean', 'count'], 'book_id': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "user_stats.columns = ['user_id', 'avg_user_rating', 'ratings_count', 'unique_books_count']  \n",
    "\n",
    "print(f\"     avg_user_rating, ratings_count, unique_books_count\")\n",
    "\n",
    "# 2.2. Tag vocabulary size (—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–µ–≥–æ–≤ –≤ –∏—Å—Ç–æ—Ä–∏–∏)\n",
    "print(\"\\n  –í—ã—á–∏—Å–ª–µ–Ω–∏–µ tag_vocab_size...\")\n",
    "\n",
    "user_tag_vocab = {}\n",
    "\n",
    "for user_id, group in tqdm(train_df.groupby('user_id'), desc=\"     Processing users\"):\n",
    "    user_tag_set = set()\n",
    "    \n",
    "    for book_id in group['book_id']:\n",
    "        if book_id in book_tags_clean:\n",
    "            user_tag_set.update(book_tags_clean[book_id])\n",
    "    \n",
    "    user_tag_vocab[user_id] = len(user_tag_set)\n",
    "\n",
    "user_stats['tag_vocab_size'] = user_stats['user_id'].map(user_tag_vocab).fillna(0)\n",
    "\n",
    "print(f\"     tag_vocab_size\")\n",
    "\n",
    "# 2.3. Activity score (–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏)\n",
    "print(\"\\n  –í—ã—á–∏—Å–ª–µ–Ω–∏–µ activity_score...\")\n",
    "\n",
    "def minmax_scale(series):\n",
    "    \"\"\"Min-max –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\"\"\"\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    if max_val == min_val:\n",
    "        return pd.Series(np.ones(len(series)), index=series.index)\n",
    "    return (series - min_val) / (max_val - min_val)\n",
    "\n",
    "# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º ratings_count –∏ tag_vocab_size\n",
    "norm_ratings = minmax_scale(user_stats['ratings_count'])\n",
    "norm_vocab = minmax_scale(user_stats['tag_vocab_size'])\n",
    "\n",
    "# Activity = 0.7 * norm(ratings) + 0.3 * norm(vocab)\n",
    "user_stats['activity_score'] = 0.7 * norm_ratings + 0.3 * norm_vocab\n",
    "\n",
    "print(f\"      activity_score\")\n",
    "\n",
    "# 2.4. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
    "print(\"\\n  –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...\")\n",
    "\n",
    "def classify_user_segment(unique_books): \n",
    "    \"\"\"–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏\"\"\"\n",
    "    if unique_books < 5:\n",
    "        return 'new'\n",
    "    elif 5 <= unique_books <= 10:\n",
    "        return 'inactive'\n",
    "    elif 11 <= unique_books <= 60:\n",
    "        return 'active'\n",
    "    else:\n",
    "        return 'very_active'\n",
    "\n",
    "user_stats['segment'] = user_stats['unique_books_count'].apply(classify_user_segment)\n",
    "# –£–¥–∞–ª—è–µ–º unique_books_count (–Ω—É–∂–µ–Ω –±—ã–ª —Ç–æ–ª—å–∫–æ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏)\n",
    "user_stats = user_stats.drop(columns=['unique_books_count'])\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º\n",
    "print(f\"\\n   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º:\")\n",
    "segment_counts = user_stats['segment'].value_counts()\n",
    "for segment, count in segment_counts.items():\n",
    "    pct = count / len(user_stats) * 100\n",
    "    print(f\"     ‚Ä¢ {segment:15s}: {count:>6,} ({pct:>5.1f}%)\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º user features\n",
    "user_stats.to_csv('user_features.csv', index=False)\n",
    "print(f\"\\n   User features —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: user_features.csv\")\n",
    "print(f\"     –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(user_stats.columns) - 1} (–∫—Ä–æ–º–µ user_id)\")\n",
    "\n",
    "# ================================================================================\n",
    "# 3. BOOK FEATURES (3 –ø—Ä–∏–∑–Ω–∞–∫–∞)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[3/6] –°–û–ó–î–ê–ù–ò–ï BOOK FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n  –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –∫–Ω–∏–≥...\")\n",
    "\n",
    "# 3.1. –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "book_stats = train_df.groupby('book_id').agg({\n",
    "    'rating': ['mean', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "book_stats.columns = ['book_id', 'book_avg_rating', 'book_ratings_count']\n",
    "\n",
    "print(f\"      book_avg_rating, book_ratings_count\")\n",
    "\n",
    "# 3.2. Book popularity (–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)\n",
    "print(\"\\n  –í—ã—á–∏—Å–ª–µ–Ω–∏–µ book_popularity...\")\n",
    "\n",
    "# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º\n",
    "norm_book_ratings = minmax_scale(book_stats['book_ratings_count'])\n",
    "norm_book_avg = minmax_scale(book_stats['book_avg_rating'])\n",
    "\n",
    "# Popularity = 0.7 * norm(ratings_count) + 0.3 * norm(avg_rating)\n",
    "book_stats['book_popularity'] = 0.7 * norm_book_ratings + 0.3 * norm_book_avg\n",
    "\n",
    "print(f\"     book_popularity\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º book features\n",
    "book_stats.to_csv('book_features.csv', index=False)\n",
    "print(f\"\\n  Book features —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: book_features.csv\")\n",
    "print(f\"     –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(book_stats.columns) - 1} (–∫—Ä–æ–º–µ book_id)\")\n",
    "\n",
    "# ================================================================================\n",
    "# 4. INTERACTION FEATURES (6 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤) \n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4/6] –°–û–ó–î–ê–ù–ò–ï INTERACTION FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n –≠—Ç–æ –∑–∞–π–º–µ—Ç –≤—Ä–µ–º—è (–º–Ω–æ–≥–æ –ø–∞—Ä user-book)...\")\n",
    "print(f\"  –ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è: {len(train_df):,} train + {len(test_df):,} test\")\n",
    "\n",
    "# –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ 768\n",
    "EMB_DIM = 768\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –ë–ï–ó data leakage –¥–ª—è train\n",
    "def create_interaction_features_train(idx, user_id, book_id, train_df, book_tags_clean, \n",
    "                                       user_embeddings, book_embeddings):\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–µ—Ç 6 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è TRAIN\n",
    "    –ò—Å–∫–ª—é—á–∞–µ—Ç —Ç–µ–∫—É—â—É—é –∫–Ω–∏–≥—É –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è!\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # --- Tag-based features (–ë–ï–ó —Ç–µ–∫—É—â–µ–π –∫–Ω–∏–≥–∏ –≤ –∏—Å—Ç–æ—Ä–∏–∏) ---\n",
    "    # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —é–∑–µ—Ä–∞ –î–û —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–æ–∫–∏ (–ø–æ –∏–Ω–¥–µ–∫—Å—É)\n",
    "    user_history = train_df[(train_df['user_id'] == user_id) & (train_df.index < idx)]\n",
    "    \n",
    "    user_tags = set()\n",
    "    for bid in user_history['book_id']:\n",
    "        if bid in book_tags_clean:\n",
    "            user_tags.update(book_tags_clean[bid])\n",
    "    \n",
    "    book_tags = set(book_tags_clean.get(book_id, []))\n",
    "    \n",
    "    # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤\n",
    "    intersection = user_tags & book_tags\n",
    "    union = user_tags | book_tags\n",
    "    \n",
    "    features['tag_overlap_count'] = len(intersection)\n",
    "    features['tag_overlap_ratio'] = len(intersection) / len(book_tags) if book_tags else 0\n",
    "    features['tag_jaccard'] = len(intersection) / len(union) if union else 0\n",
    "    features['history_similarity'] = 0.6 * features['tag_overlap_ratio'] + 0.4 * features['tag_jaccard']\n",
    "    \n",
    "    # --- Embedding-based features ---\n",
    "    user_emb = user_embeddings.get(user_id, np.zeros(EMB_DIM))\n",
    "    book_emb = book_embeddings.get(book_id, np.zeros(EMB_DIM))\n",
    "    \n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω—É–ª–µ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
    "    user_norm = np.linalg.norm(user_emb)\n",
    "    book_norm = np.linalg.norm(book_emb)\n",
    "    \n",
    "    if user_norm == 0 or book_norm == 0:\n",
    "        features['embedding_cosine_sim'] = 0.0\n",
    "        features['embedding_euclidean_dist'] = 1.0  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ\n",
    "    else:\n",
    "        # –ö–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å\n",
    "        features['embedding_cosine_sim'] = np.dot(user_emb, book_emb) / (user_norm * book_norm)\n",
    "        \n",
    "        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 7: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –µ–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ\n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä—ã\n",
    "        user_emb_norm = user_emb / user_norm\n",
    "        book_emb_norm = book_emb / book_norm\n",
    "        features['embedding_euclidean_dist'] = np.linalg.norm(user_emb_norm - book_emb_norm) / np.sqrt(2)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è test (–∏—Å–ø–æ–ª—å–∑—É–µ–º –í–°–Æ –∏—Å—Ç–æ—Ä–∏—é train)\n",
    "def create_interaction_features_test(user_id, book_id, user_tags_dict, book_tags_clean, \n",
    "                                      user_embeddings, book_embeddings):\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–µ—Ç 6 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è TEST\n",
    "    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–ª–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ train\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # --- Tag-based features ---\n",
    "    user_tags = user_tags_dict.get(user_id, set())\n",
    "    book_tags = set(book_tags_clean.get(book_id, []))\n",
    "    \n",
    "    intersection = user_tags & book_tags\n",
    "    union = user_tags | book_tags\n",
    "    \n",
    "    features['tag_overlap_count'] = len(intersection)\n",
    "    features['tag_overlap_ratio'] = len(intersection) / len(book_tags) if book_tags else 0\n",
    "    features['tag_jaccard'] = len(intersection) / len(union) if union else 0\n",
    "    features['history_similarity'] = 0.6 * features['tag_overlap_ratio'] + 0.4 * features['tag_jaccard']\n",
    "    \n",
    "    # --- Embedding-based features ---\n",
    "    user_emb = user_embeddings.get(user_id, np.zeros(EMB_DIM))\n",
    "    book_emb = book_embeddings.get(book_id, np.zeros(EMB_DIM))\n",
    "    \n",
    "    user_norm = np.linalg.norm(user_emb)\n",
    "    book_norm = np.linalg.norm(book_emb)\n",
    "    \n",
    "    if user_norm == 0 or book_norm == 0:\n",
    "        features['embedding_cosine_sim'] = 0.0\n",
    "        features['embedding_euclidean_dist'] = 1.0\n",
    "    else:\n",
    "        features['embedding_cosine_sim'] = np.dot(user_emb, book_emb) / (user_norm * book_norm)\n",
    "        \n",
    "        user_emb_norm = user_emb / user_norm\n",
    "        book_emb_norm = book_emb / book_norm\n",
    "        features['embedding_euclidean_dist'] = np.linalg.norm(user_emb_norm - book_emb_norm) / np.sqrt(2)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º interaction features –¥–ª—è TRAIN\n",
    "print(f\"\\n  –°–æ–∑–¥–∞–Ω–∏–µ interaction features –¥–ª—è train...\")\n",
    "\n",
    "train_interactions = []\n",
    "\n",
    "for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"     Train\"):\n",
    "    features = create_interaction_features_train(\n",
    "        idx,\n",
    "        row['user_id'], \n",
    "        row['book_id'],\n",
    "        train_df,\n",
    "        book_tags_clean,\n",
    "        user_embeddings_train,\n",
    "        book_embeddings\n",
    "    )\n",
    "    \n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è\n",
    "    features['user_id'] = row['user_id']\n",
    "    features['book_id'] = row['book_id']\n",
    "    features['rating'] = row['rating']\n",
    "    \n",
    "    train_interactions.append(features)\n",
    "\n",
    "train_features_df = pd.DataFrame(train_interactions)\n",
    "\n",
    "print(f\"  Train interaction features: {len(train_features_df):,}\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å user_tags –¥–ª—è TEST (–ø–æ–ª–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è train)\n",
    "print(f\"\\n  –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ user tags –¥–ª—è test...\")\n",
    "\n",
    "user_tags_dict = {}\n",
    "\n",
    "for user_id, group in tqdm(train_df.groupby('user_id'), desc=\"     User tags\"):\n",
    "    user_tags = set()\n",
    "    for book_id in group['book_id']:\n",
    "        if book_id in book_tags_clean:\n",
    "            user_tags.update(book_tags_clean[book_id])\n",
    "    user_tags_dict[user_id] = user_tags\n",
    "\n",
    "print(f\"  User tags –≥–æ—Ç–æ–≤—ã: {len(user_tags_dict):,} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º interaction features –¥–ª—è TEST\n",
    "print(f\"\\n  –°–æ–∑–¥–∞–Ω–∏–µ interaction features –¥–ª—è test...\")\n",
    "\n",
    "test_interactions = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"     Test\"):\n",
    "    features = create_interaction_features_test(\n",
    "        row['user_id'], \n",
    "        row['book_id'],\n",
    "        user_tags_dict,\n",
    "        book_tags_clean,\n",
    "        user_embeddings_test,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º test embeddings!\n",
    "        book_embeddings\n",
    "    )\n",
    "    \n",
    "    features['user_id'] = row['user_id']\n",
    "    features['book_id'] = row['book_id']\n",
    "    features['rating'] = row['rating']\n",
    "    \n",
    "    test_interactions.append(features)\n",
    "\n",
    "test_features_df = pd.DataFrame(test_interactions)\n",
    "\n",
    "print(f\"  Test interaction features: {len(test_features_df):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9a7705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[5/6] –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –í–°–ï–• –ü–†–ò–ó–ù–ê–ö–û–í\n",
      "================================================================================\n",
      "\n",
      "  –ú–µ—Ä–¥–∂ —Å user –∏ book features...\n",
      "     Train: (874496, 17)\n",
      "     Test: (107260, 17)\n",
      "\n",
      "  –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ preprocessing...\n",
      "     –î–æ–±–∞–≤–ª–µ–Ω–æ 4 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ preprocessing\n",
      "      Train final: (874496, 21)\n",
      "     Test final: (107260, 21)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================================================================\n",
    "# 5. –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –í–°–ï–• –ü–†–ò–ó–ù–ê–ö–û–í\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5/6] –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –í–°–ï–• –ü–†–ò–ó–ù–ê–ö–û–í\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n  –ú–µ—Ä–¥–∂ —Å user –∏ book features...\")\n",
    "\n",
    "# Train\n",
    "train_full = train_features_df.merge(user_stats, on='user_id', how='left')\n",
    "train_full = train_full.merge(book_stats, on='book_id', how='left')\n",
    "\n",
    "# Test\n",
    "test_full = test_features_df.merge(user_stats, on='user_id', how='left')\n",
    "test_full = test_full.merge(book_stats, on='book_id', how='left')\n",
    "\n",
    "print(f\"     Train: {train_full.shape}\")\n",
    "print(f\"     Test: {test_full.shape}\")\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ preprocessing\n",
    "print(\"\\n  –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ preprocessing...\")\n",
    "\n",
    "# –ü—Ä–∏–∑–Ω–∞–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å\n",
    "extra_cols = ['language_code_encoded', 'year_normalized', 'publication_era', 'average_rating']\n",
    "\n",
    "# –î–ª—è train\n",
    "train_extra = train_df[['user_id', 'book_id'] + extra_cols].copy()\n",
    "# –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á (–¥–ª—è —Å–ª—É—á–∞—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)\n",
    "train_extra['row_num'] = train_extra.groupby(['user_id', 'book_id']).cumcount()\n",
    "train_full['row_num'] = train_full.groupby(['user_id', 'book_id']).cumcount()\n",
    "\n",
    "train_full = train_full.merge(train_extra, on=['user_id', 'book_id', 'row_num'], how='left')\n",
    "train_full = train_full.drop(columns=['row_num'])\n",
    "\n",
    "# –î–ª—è test\n",
    "test_extra = test_df[['user_id', 'book_id'] + extra_cols].copy()\n",
    "test_extra['row_num'] = test_extra.groupby(['user_id', 'book_id']).cumcount()\n",
    "test_full['row_num'] = test_full.groupby(['user_id', 'book_id']).cumcount()\n",
    "\n",
    "test_full = test_full.merge(test_extra, on=['user_id', 'book_id', 'row_num'], how='left')\n",
    "test_full = test_full.drop(columns=['row_num'])\n",
    "\n",
    "print(f\"     –î–æ–±–∞–≤–ª–µ–Ω–æ {len(extra_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ preprocessing\")\n",
    "print(f\"      Train final: {train_full.shape}\")\n",
    "print(f\"     Test final: {test_full.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41b4f92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤...\n",
      "      Train: –ø—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ—Ç\n",
      "   Test –ø—Ä–æ–ø—É—Å–∫–∏ (cold start books):\n",
      "book_avg_rating       400\n",
      "book_ratings_count    400\n",
      "book_popularity       400\n",
      "dtype: int64\n",
      "\n",
      "      –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤...\n",
      "      –ó–∞–ø–æ–ª–Ω–µ–Ω–æ 1200 –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
      "      –í—Å–µ –ø—Ä–æ–ø—É—Å–∫–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_18488\\3582154444.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_full['book_avg_rating'].fillna(train_full['book_avg_rating'].median(), inplace=True)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_18488\\3582154444.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_full['book_ratings_count'].fillna(1, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   –ü–æ–ª–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:\n",
      "     ‚Ä¢ train_features_full.csv\n",
      "     ‚Ä¢ test_features_full.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
    "print(\"\\n  üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤...\")\n",
    "train_missing = train_full.isnull().sum()\n",
    "test_missing = test_full.isnull().sum()\n",
    "\n",
    "if train_missing.sum() > 0:\n",
    "    print(f\"       Train –ø—Ä–æ–ø—É—Å–∫–∏:\")\n",
    "    print(train_missing[train_missing > 0])\n",
    "else:\n",
    "    print(f\"      Train: –ø—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ—Ç\")\n",
    "\n",
    "if test_missing.sum() > 0:\n",
    "    print(f\"   Test –ø—Ä–æ–ø—É—Å–∫–∏ (cold start books):\")\n",
    "    print(test_missing[test_missing > 0])\n",
    "    \n",
    "    print(f\"\\n      –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤...\")\n",
    "    \n",
    "    # 1. book_avg_rating: –∏—Å–ø–æ–ª—å–∑—É–µ–º average_rating –∏–∑ Goodreads\n",
    "    mask = test_full['book_avg_rating'].isna()\n",
    "    if mask.sum() > 0:\n",
    "        test_full.loc[mask, 'book_avg_rating'] = test_full.loc[mask, 'average_rating']\n",
    "    \n",
    "    # 2. –ï—Å–ª–∏ –∏ Goodreads –Ω–µ—Ç - –≥–ª–æ–±–∞–ª—å–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ\n",
    "    test_full['book_avg_rating'].fillna(train_full['book_avg_rating'].median(), inplace=True)\n",
    "    \n",
    "    # 3. book_ratings_count = –º–∏–Ω–∏–º—É–º (–Ω–æ–≤–∞—è –∫–Ω–∏–≥–∞)\n",
    "    test_full['book_ratings_count'].fillna(1, inplace=True)\n",
    "    \n",
    "    # 4. book_popularity: –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–ª—è —Ö–æ–ª–æ–¥–Ω—ã—Ö –∫–Ω–∏–≥\n",
    "    mask = test_full['book_popularity'].isna()\n",
    "    if mask.sum() > 0:\n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ train\n",
    "        max_ratings = train_full['book_ratings_count'].max()\n",
    "        min_rating = train_full['book_avg_rating'].min()\n",
    "        max_rating = train_full['book_avg_rating'].max()\n",
    "        \n",
    "        norm_count = (test_full.loc[mask, 'book_ratings_count'] - 1) / max_ratings\n",
    "        norm_rating = (test_full.loc[mask, 'book_avg_rating'] - min_rating) / (max_rating - min_rating)\n",
    "        test_full.loc[mask, 'book_popularity'] = 0.7 * norm_count + 0.3 * norm_rating\n",
    "    \n",
    "    print(f\"      –ó–∞–ø–æ–ª–Ω–µ–Ω–æ {test_missing[test_missing > 0].sum()} –ø—Ä–æ–ø—É—Å–∫–æ–≤\")\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å\n",
    "    feature_cols = [\n",
    "    # Interaction (6)\n",
    "    'tag_overlap_count', 'tag_overlap_ratio', 'tag_jaccard', \n",
    "    'history_similarity', 'embedding_cosine_sim', 'embedding_euclidean_dist',\n",
    "    # User (4) - –ë–ï–ó unique_books_count!\n",
    "    'avg_user_rating', 'ratings_count', 'tag_vocab_size', 'activity_score',\n",
    "    # Book (3)\n",
    "    'book_avg_rating', 'book_ratings_count', 'book_popularity',\n",
    "    # Preprocessing (4)\n",
    "    'language_code_encoded', 'year_normalized', 'publication_era', 'average_rating'\n",
    "]\n",
    "    test_missing_after = test_full[feature_cols].isnull().sum()\n",
    "    if test_missing_after.sum() > 0:\n",
    "        print(f\"       –û—Å—Ç–∞–ª–∏—Å—å –ø—Ä–æ–ø—É—Å–∫–∏:\")\n",
    "        print(test_missing_after[test_missing_after > 0])\n",
    "    else:\n",
    "        print(f\"      –í—Å–µ –ø—Ä–æ–ø—É—Å–∫–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã!\")\n",
    "else:\n",
    "    print(f\"  Test: –ø—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ—Ç\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º\n",
    "train_full.to_csv('train_features_full.csv', index=False)\n",
    "test_full.to_csv('test_features_full.csv', index=False)\n",
    "\n",
    "print(f\"\\n   –ü–æ–ª–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:\")\n",
    "print(f\"     ‚Ä¢ train_features_full.csv\")\n",
    "print(f\"     ‚Ä¢ test_features_full.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "333d8122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[6/6] –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê\n",
      "================================================================================\n",
      "\n",
      " –°–æ–∑–¥–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:\n",
      "\n",
      "   Interaction Features (6):\n",
      "     ‚Ä¢ tag_overlap_count - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö —Ç–µ–≥–æ–≤\n",
      "     ‚Ä¢ tag_overlap_ratio - –¥–æ–ª—è –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è\n",
      "     ‚Ä¢ tag_jaccard - –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞\n",
      "     ‚Ä¢ history_similarity - –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å\n",
      "     ‚Ä¢ embedding_cosine_sim - –∫–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å (768-dim)\n",
      "     ‚Ä¢ embedding_euclidean_dist - –µ–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ)\n",
      "\n",
      "   User Features (4):\n",
      "     ‚Ä¢ avg_user_rating - —Å—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
      "     ‚Ä¢ ratings_count - –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å\n",
      "     ‚Ä¢ tag_vocab_size - —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤\n",
      "     ‚Ä¢ activity_score - –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞\n",
      "\n",
      "   Book Features (3):\n",
      "     ‚Ä¢ book_avg_rating - —Å—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –∫–Ω–∏–≥–∏\n",
      "     ‚Ä¢ book_ratings_count - –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç—å\n",
      "     ‚Ä¢ book_popularity - –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å\n",
      "\n",
      "   From Preprocessing (4):\n",
      "     ‚Ä¢ language_code_encoded - —è–∑—ã–∫ –∫–Ω–∏–≥–∏ (–∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω)\n",
      "     ‚Ä¢ year_normalized - –≥–æ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω)\n",
      "     ‚Ä¢ publication_era - —ç–ø–æ—Ö–∞ (0-7)\n",
      "     ‚Ä¢ average_rating - —Å—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –∏–∑ Goodreads\n",
      "\n",
      "   –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ:\n",
      "     ‚Ä¢ segment - –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π (new/inactive/active/very_active)\n",
      "\n",
      "   –í—Å–µ–≥–æ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: 17\n",
      "\n",
      " –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (train):\n",
      "\n",
      "        tag_overlap_count  tag_overlap_ratio    tag_jaccard  \\\n",
      "count      874496.000000      874496.000000  874496.000000   \n",
      "mean           71.698926           0.723241       0.152782   \n",
      "std            26.365283           0.264414       0.104213   \n",
      "min             0.000000           0.000000       0.000000   \n",
      "25%            61.000000           0.618557       0.087935   \n",
      "50%            81.000000           0.810000       0.130990   \n",
      "75%            91.000000           0.919192       0.194154   \n",
      "max           100.000000           1.000000       1.000000   \n",
      "\n",
      "       history_similarity  embedding_cosine_sim  embedding_euclidean_dist  \\\n",
      "count       874496.000000         874496.000000              8.744960e+05   \n",
      "mean             0.495057              0.751734              4.913922e-01   \n",
      "std              0.173495              0.079743              8.245769e-02   \n",
      "min              0.000000              0.000000              1.670706e-08   \n",
      "25%              0.438283              0.706297              4.443083e-01   \n",
      "50%              0.548303              0.758557              4.913685e-01   \n",
      "75%              0.608499              0.802590              5.419433e-01   \n",
      "max              1.000000              1.000000              1.000000e+00   \n",
      "\n",
      "       avg_user_rating  ratings_count  tag_vocab_size  activity_score  \\\n",
      "count    874496.000000  874496.000000   874496.000000   874496.000000   \n",
      "mean          3.857356      56.740010      948.196812        0.296525   \n",
      "std           0.541612      49.238319      501.362532        0.227060   \n",
      "min           1.000000       2.000000       55.000000        0.000000   \n",
      "25%           3.500000      19.000000      576.000000        0.121521   \n",
      "50%           3.848485      42.000000      883.000000        0.243036   \n",
      "75%           4.214286      81.000000     1263.000000        0.418796   \n",
      "max           5.000000     197.000000     2734.000000        0.974872   \n",
      "\n",
      "       book_avg_rating  book_ratings_count  book_popularity  \\\n",
      "count    874496.000000       874496.000000    874496.000000   \n",
      "mean          3.857356           93.507673         0.841349   \n",
      "std           0.302491           14.511467         0.106451   \n",
      "min           1.959596            1.000000         0.003987   \n",
      "25%           3.670000           95.000000         0.836206   \n",
      "50%           3.875000          100.000000         0.873701   \n",
      "75%           4.060000          100.000000         0.897382   \n",
      "max           5.000000          100.000000         0.978292   \n",
      "\n",
      "       language_code_encoded  year_normalized  publication_era  average_rating  \n",
      "count          874496.000000    874496.000000    874496.000000    874496.00000  \n",
      "mean                5.765791      3724.135167         6.456219         4.00594  \n",
      "std                 1.175623       225.028724         0.899750         0.25185  \n",
      "min                 0.000000         0.000000         0.000000         2.47000  \n",
      "25%                 6.000000      3740.000000         6.000000         3.85000  \n",
      "50%                 6.000000      3754.000000         7.000000         4.02000  \n",
      "75%                 6.000000      3761.000000         7.000000         4.18000  \n",
      "max                24.000000      3767.000000         7.000000         4.82000  \n",
      "\n",
      "   –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: features_metadata.pkl\n",
      "\n",
      "================================================================================\n",
      " FEATURE ENGINEERING –ó–ê–í–ï–†–®–ï–ù!\n",
      "================================================================================\n",
      "\n",
      " –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:\n",
      "  1. user_features.csv\n",
      "  2. book_features.csv\n",
      "  3. train_features_full.csv\n",
      "  4. test_features_full.csv\n",
      "  5. features_metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================================================================\n",
    "# 6. –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6/6] –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "print(f\"\\n –°–æ–∑–¥–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:\")\n",
    "\n",
    "print(f\"\\n   Interaction Features (6):\")\n",
    "print(f\"     ‚Ä¢ tag_overlap_count - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö —Ç–µ–≥–æ–≤\")\n",
    "print(f\"     ‚Ä¢ tag_overlap_ratio - –¥–æ–ª—è –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è\")\n",
    "print(f\"     ‚Ä¢ tag_jaccard - –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞\")\n",
    "print(f\"     ‚Ä¢ history_similarity - –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å\")\n",
    "print(f\"     ‚Ä¢ embedding_cosine_sim - –∫–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å (768-dim)\")\n",
    "print(f\"     ‚Ä¢ embedding_euclidean_dist - –µ–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ)\")\n",
    "\n",
    "print(f\"\\n   User Features (4):\")\n",
    "print(f\"     ‚Ä¢ avg_user_rating - —Å—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\")\n",
    "print(f\"     ‚Ä¢ ratings_count - –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å\")\n",
    "print(f\"     ‚Ä¢ tag_vocab_size - —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤\")\n",
    "print(f\"     ‚Ä¢ activity_score - –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞\")\n",
    "\n",
    "print(f\"\\n   Book Features (3):\")\n",
    "print(f\"     ‚Ä¢ book_avg_rating - —Å—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –∫–Ω–∏–≥–∏\")\n",
    "print(f\"     ‚Ä¢ book_ratings_count - –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç—å\")\n",
    "print(f\"     ‚Ä¢ book_popularity - –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å\")\n",
    "\n",
    "print(f\"\\n   From Preprocessing (4):\")\n",
    "print(f\"     ‚Ä¢ language_code_encoded - —è–∑—ã–∫ –∫–Ω–∏–≥–∏ (–∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω)\")\n",
    "print(f\"     ‚Ä¢ year_normalized - –≥–æ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω)\")\n",
    "print(f\"     ‚Ä¢ publication_era - —ç–ø–æ—Ö–∞ (0-7)\")\n",
    "print(f\"     ‚Ä¢ average_rating - —Å—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –∏–∑ Goodreads\")\n",
    "\n",
    "print(f\"\\n   –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ:\")\n",
    "print(f\"     ‚Ä¢ segment - –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π (new/inactive/active/very_active)\")\n",
    "\n",
    "total_features = len(feature_cols)\n",
    "print(f\"\\n   –í—Å–µ–≥–æ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {total_features}\")\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º\n",
    "print(f\"\\n –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (train):\")\n",
    "print(\"\\n\", train_full[feature_cols].describe())\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\n",
    "metadata = {\n",
    "    'num_users': len(user_stats),\n",
    "    'num_books': len(book_stats),\n",
    "    'train_size': len(train_full),\n",
    "    'test_size': len(test_full),\n",
    "    'num_features': total_features,\n",
    "    'feature_names': feature_cols,\n",
    "    'interaction_features': feature_cols[:6],\n",
    "    'user_features': feature_cols[6:10],\n",
    "    'book_features': feature_cols[10:13],\n",
    "    'preprocessing_features': feature_cols[13:17],\n",
    "    'segment_distribution': segment_counts.to_dict(),\n",
    "    'embedding_dim': EMB_DIM\n",
    "}\n",
    "\n",
    "with open('features_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(f\"\\n   –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: features_metadata.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" FEATURE ENGINEERING –ó–ê–í–ï–†–®–ï–ù!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:\")\n",
    "print(f\"  1. user_features.csv\")\n",
    "print(f\"  2. book_features.csv\")\n",
    "print(f\"  3. train_features_full.csv\")\n",
    "print(f\"  4. test_features_full.csv\")\n",
    "print(f\"  5. features_metadata.pkl\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl_env_cuda)",
   "language": "python",
   "name": "dl_env_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
