{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9738003d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPROCESSING STARTED\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# PREPROCESSING: –ü–û–õ–ù–´–ô –¶–ò–ö–õ \n",
    "# ================================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PREPROCESSING STARTED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "drive_path = './'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acc380eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/9] –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
      "   –ó–∞–≥—Ä—É–∂–µ–Ω–æ: ratings=981,756, books=10,000, tags=34,252, book_tags=999,912\n",
      "\n",
      "[2/9] –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤...\n",
      "  –í—Å–µ —Ä–µ–π—Ç–∏–Ω–≥–∏ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞\n",
      "   –ù–∞–π–¥–µ–Ω–æ 4487 –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (user_id, book_id) - –æ—Å—Ç–∞–≤–ª—è–µ–º –≤—Å–µ –æ—Ü–µ–Ω–∫–∏\n",
      "  –ò—Ç–æ–≥–æ ratings: 981,756 –æ—Ü–µ–Ω–æ–∫\n",
      "    –£–¥–∞–ª–µ–Ω–æ 6 –∑–∞–ø–∏—Å–µ–π —Å count < 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================================================================\n",
    "# 1. –ó–ê–ì–†–£–ó–ö–ê –ò –ë–ê–ó–û–í–ê–Ø –û–ß–ò–°–¢–ö–ê\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n[1/9] –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "ratings_df = pd.read_csv(f'{drive_path}/ratings.csv')\n",
    "books_df = pd.read_csv(f'{drive_path}/books.csv')\n",
    "tags_df = pd.read_csv(f'{drive_path}/tags.csv')\n",
    "book_tags_df = pd.read_csv(f'{drive_path}/book_tags.csv')\n",
    "\n",
    "print(f\"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ: ratings={len(ratings_df):,}, books={len(books_df):,}, tags={len(tags_df):,}, book_tags={len(book_tags_df):,}\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤\n",
    "print(\"\\n[2/9] –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤...\")\n",
    "invalid_ratings = (ratings_df['rating'] < 1) | (ratings_df['rating'] > 5)\n",
    "fractional_ratings = (ratings_df['rating'] % 1 != 0)\n",
    "\n",
    "if invalid_ratings.sum() > 0:\n",
    "    print(f\"    –ù–∞–π–¥–µ–Ω–æ {invalid_ratings.sum()} –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ - —É–¥–∞–ª—è–µ–º\")\n",
    "    ratings_df = ratings_df[~invalid_ratings]\n",
    "\n",
    "if fractional_ratings.sum() > 0:\n",
    "    print(f\"   –ù–∞–π–¥–µ–Ω–æ {fractional_ratings.sum()} –¥—Ä–æ–±–Ω—ã—Ö —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ - –æ–∫—Ä—É–≥–ª—è–µ–º\")\n",
    "    ratings_df['rating'] = ratings_df['rating'].round().astype(int)\n",
    "else:\n",
    "    print(f\"  –í—Å–µ —Ä–µ–π—Ç–∏–Ω–≥–∏ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (–Ω–æ –ù–ï —É–¥–∞–ª—è–µ–º!)\n",
    "duplicates = ratings_df.duplicated(subset=['user_id', 'book_id'], keep=False).sum()\n",
    "print(f\"   –ù–∞–π–¥–µ–Ω–æ {duplicates} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (user_id, book_id) - –æ—Å—Ç–∞–≤–ª—è–µ–º –≤—Å–µ –æ—Ü–µ–Ω–∫–∏\")\n",
    "\n",
    "print(f\"  –ò—Ç–æ–≥–æ ratings: {len(ratings_df):,} –æ—Ü–µ–Ω–æ–∫\")\n",
    "\n",
    "# –û—á–∏—Å—Ç–∫–∞ book_tags\n",
    "neg_count = (book_tags_df['count'] < 0).sum()\n",
    "if neg_count > 0:\n",
    "    print(f\"    –£–¥–∞–ª–µ–Ω–æ {neg_count} –∑–∞–ø–∏—Å–µ–π —Å count < 0\")\n",
    "    book_tags_df = book_tags_df[book_tags_df['count'] > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f62812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/9] –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–æ–¥–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏...\n",
      "   –ö–Ω–∏–≥ —Å –≥–æ–¥–æ–º < 0: 31\n",
      "    –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≥–æ–¥–∞: 21\n",
      "    –î–∏–∞–ø–∞–∑–æ–Ω –≥–æ–¥–æ–≤: -1750 - 2017\n",
      "   –°–æ–∑–¥–∞–Ω–æ:\n",
      "     ‚Ä¢ publication_era (0-7): {0: 21, 1: 31, 2: 19, 3: 75, 4: 254, 5: 466, 6: 2946, 7: 6188}\n",
      "     ‚Ä¢ year_normalized (0-3767)\n",
      "\n",
      "[4/9] –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π...\n",
      "    –ü—Ä–æ–ø—É—Å–∫–∏:\n",
      "     ‚Ä¢ title: 0\n",
      "     ‚Ä¢ authors: 0\n",
      "     ‚Ä¢ language_code: 1084\n",
      "   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: title, authors, language_code\n",
      "   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤: 25\n",
      "   language_code_encoded: –¥–∏–∞–ø–∞–∑–æ–Ω 0-24\n",
      "\n",
      "[5/9] –°–æ–∑–¥–∞–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ ID...\n",
      "   –ú–∞–ø–ø–∏–Ω–≥ —Å–æ–∑–¥–∞–Ω: 10,000 –∫–Ω–∏–≥\n",
      "\n",
      "[6/9] –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–≥–æ–≤...\n",
      "   –ß–∏—Å—Ç—ã—Ö —Ç–µ–≥–æ–≤: 32,672 / 34,252\n",
      "   –ó–∞–ø–∏—Å–µ–π book_tags –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: 989,657\n",
      "  –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–æ –í–°–ï–• —Ç–µ–≥–æ–≤ –¥–ª—è 10,000 –∫–Ω–∏–≥ (–ë–ï–ó –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç–æ–ø-20)\n",
      "   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–≥–æ–≤ –Ω–∞ –∫–Ω–∏–≥—É: 99.0\n",
      "   –ü–æ—Å–ª–µ –º–∞–ø–ø–∏–Ω–≥–∞: 10,000 –∫–Ω–∏–≥\n",
      "\n",
      "[7/9] –°–æ–∑–¥–∞–Ω–∏–µ –µ–¥–∏–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...\n",
      "   –ú–µ—Ä–¥–∂ —Å books: 981,756 ‚Üí 981,756 (–ø–æ—Ç–µ—Ä—è–Ω–æ: 0)\n",
      "   –ú–µ—Ä–¥–∂ —Å tags: 981,756 ‚Üí 981,756 (–ø–æ—Ç–µ—Ä—è–Ω–æ: 0)\n",
      "   Complete dataset: 981,756 —Å—Ç—Ä–æ–∫ √ó 12 –∫–æ–ª–æ–Ω–æ–∫\n",
      "   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–Ω–∏–≥: 10,000\n",
      "   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: 53,424\n",
      "   –ö–Ω–∏–≥ —Å —Ç–µ–≥–∞–º–∏: 981,756\n",
      "\n",
      "[8/9] Train/Test split (temporal + stratified)...\n",
      "   Train: 874,496 –æ—Ü–µ–Ω–æ–∫\n",
      "   Test: 107,260 –æ—Ü–µ–Ω–æ–∫\n",
      "   –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ test: 35,659 / 53,424\n",
      "    –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Ç–æ–ª—å–∫–æ –≤ train (< 5 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–Ω–∏–≥): 17,765\n",
      "    –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: 2,278\n",
      "   Data leakage check: OK (–Ω–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –ø–æ (user_id, book_id))\n",
      "    –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤:\n",
      "     Train: {1: 17504, 2: 56212, 3: 221314, 4: 317958, 5: 261508}\n",
      "     Test:  {1: 2071, 2: 7019, 3: 27309, 4: 39408, 5: 31453}\n",
      "\n",
      "[9/10] –°–æ–∑–¥–∞–Ω–∏–µ embeddings...\n",
      "  üñ•Ô∏è  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: CUDA\n",
      "   –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (dim=384)\n",
      "   –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–≥–æ–≤ –¥–ª—è 10,000 –∫–Ω–∏–≥...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8e25c3edb049c3ab160c01b4c49bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Book tag embeddings: 10,000 √ó 384\n",
      "   –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ title + authors –¥–ª—è 10,000 –∫–Ω–∏–≥...\n",
      "   –ü—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞:\n",
      "     ‚Ä¢ The Hunger Games (The Hunger Games, #1) by Suzanne Collins...\n",
      "     ‚Ä¢ Me Talk Pretty One Day by David Sedaris...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec56a20e18a8482f995ab7b98d734e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Book text embeddings: 10,000 √ó 384\n",
      "   –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö book embeddings...\n",
      "   Combined book embeddings: 10,000 √ó 768\n",
      "   User embeddings...\n",
      "   Train user embeddings...\n",
      "     10000/53,424...\n",
      "     20000/53,424...\n",
      "     30000/53,424...\n",
      "     40000/53,424...\n",
      "     50000/53,424...\n",
      "   Train user embeddings: 53,424 √ó 768\n",
      "   Test user embeddings...\n",
      "     10000/35,659...\n",
      "     20000/35,659...\n",
      "     30000/35,659...\n",
      "   Test user embeddings: 35,659 √ó 768\n",
      "\n",
      "[10/10] –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ...\n",
      "   tags_text —É–¥–∞–ª–µ–Ω –∏–∑ train/test (–æ—Å—Ç–∞–≤–ª–µ–Ω –≤ complete)\n",
      "   Train final: (874496, 11)\n",
      "   Test final: (107260, 11)\n",
      "   Complete: (981756, 12)\n",
      "\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...\n",
      "  [1/8] complete_dataset.csv...\n",
      "  [2/8] train_dataset.csv...\n",
      "  [3/8] test_dataset.csv...\n",
      "  [4/8] train_embeddings.pkl...\n",
      "  [5/8] test_embeddings.pkl...\n",
      "  [6/8] –°–ª–æ–≤–∞—Ä–∏ embeddings...\n",
      "  [7/8] holdout_dict.pkl...\n",
      "  [8/8] preprocessing_metadata.pkl...\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING –ó–ê–í–ï–†–®–ï–ù\n",
      "================================================================================\n",
      "\n",
      "–°–æ–∑–¥–∞–Ω–æ 8 —Ñ–∞–π–ª–æ–≤:\n",
      "  CSV:\n",
      "    ‚Ä¢ complete_dataset.csv: 981,756 √ó 12 (—Å tags_text)\n",
      "    ‚Ä¢ train_dataset.csv: 874,496 √ó 11 (–±–µ–∑ tags_text)\n",
      "    ‚Ä¢ test_dataset.csv: 107,260 √ó 11 (–±–µ–∑ tags_text)\n",
      "  PKL:\n",
      "    ‚Ä¢ train_embeddings.pkl: book (874496, 768) + user (874496, 768) + language\n",
      "    ‚Ä¢ test_embeddings.pkl: book (107260, 768) + user (107260, 768) + language\n",
      "    ‚Ä¢ book_embeddings.pkl: 3 —Ç–∏–ø–∞ (combined 768, tags 384, text 384)\n",
      "    ‚Ä¢ user_embeddings.pkl: train 53,424 + test 35,659 (768-dim)\n",
      "    ‚Ä¢ holdout_dict.pkl: 35,659 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
      "    ‚Ä¢ preprocessing_metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================================================================\n",
    "# 2. –û–ë–†–ê–ë–û–¢–ö–ê –ì–û–î–ê –ü–£–ë–õ–ò–ö–ê–¶–ò–ò\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n[3/9] –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–æ–¥–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏...\")\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "print(f\"   –ö–Ω–∏–≥ —Å –≥–æ–¥–æ–º < 0: {(books_df['original_publication_year'] < 0).sum()}\")\n",
    "print(f\"    –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≥–æ–¥–∞: {books_df['original_publication_year'].isna().sum()}\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ —ç–ø–æ—Ö\n",
    "def get_publication_era(year):\n",
    "    \"\"\"\n",
    "    0 = Unknown\n",
    "    1 = Ancient (< 0)\n",
    "    2 = Medieval (0-1500)\n",
    "    3 = Early Modern (1500-1800)\n",
    "    4 = 19th century (1800-1900)\n",
    "    5 = Early 20th (1900-1950)\n",
    "    6 = Late 20th (1950-2000)\n",
    "    7 = 21st century (2000+)\n",
    "    \"\"\"\n",
    "    if pd.isna(year):\n",
    "        return 0\n",
    "    if year < 0:\n",
    "        return 1\n",
    "    if year < 1500:\n",
    "        return 2\n",
    "    if year < 1800:\n",
    "        return 3\n",
    "    if year < 1900:\n",
    "        return 4\n",
    "    if year < 1950:\n",
    "        return 5\n",
    "    if year < 2000:\n",
    "        return 6\n",
    "    return 7\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫\n",
    "books_df['publication_era'] = books_df['original_publication_year'].apply(get_publication_era)\n",
    "\n",
    "# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≥–æ–¥–∞ (—Å–¥–≤–∏–≥ –º–∏–Ω–∏–º—É–º–∞ –∫ 0)\n",
    "min_year = books_df['original_publication_year'].min()  # -1750\n",
    "max_year = books_df['original_publication_year'].max()  # ~2025\n",
    "\n",
    "print(f\"    –î–∏–∞–ø–∞–∑–æ–Ω –≥–æ–¥–æ–≤: {min_year:.0f} - {max_year:.0f}\")\n",
    "\n",
    "# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º: —Å–¥–≤–∏–≥–∞–µ–º —Ç–∞–∫ —á—Ç–æ–±—ã –º–∏–Ω–∏–º—É–º —Å—Ç–∞–ª 0\n",
    "books_df['year_normalized'] = books_df['original_publication_year'] - min_year\n",
    "books_df['year_normalized'] = books_df['year_normalized'].fillna(0).astype(int)\n",
    "\n",
    "print(f\"   –°–æ–∑–¥–∞–Ω–æ:\")\n",
    "print(f\"     ‚Ä¢ publication_era (0-7): {books_df['publication_era'].value_counts().sort_index().to_dict()}\")\n",
    "print(f\"     ‚Ä¢ year_normalized (0-{int(max_year - min_year)})\")\n",
    "\n",
    "# –£–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ (—á—Ç–æ–±—ã –Ω–µ –ø—É—Ç–∞—Ç—å—Å—è —Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏ –≥–æ–¥–∞–º–∏)\n",
    "books_df = books_df.drop(columns=['original_publication_year'])\n",
    "\n",
    "# ================================================================================\n",
    "# 3. –û–ë–†–ê–ë–û–¢–ö–ê –¢–ï–ö–°–¢–û–í–´–• –ü–û–õ–ï–ô\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n[4/9] –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π...\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ–ø—É—Å–∫–∏\n",
    "print(f\"    –ü—Ä–æ–ø—É—Å–∫–∏:\")\n",
    "print(f\"     ‚Ä¢ title: {books_df['title'].isna().sum()}\")\n",
    "print(f\"     ‚Ä¢ authors: {books_df['authors'].isna().sum()}\")\n",
    "print(f\"     ‚Ä¢ language_code: {books_df['language_code'].isna().sum()}\")\n",
    "\n",
    "# title - –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏\n",
    "if books_df['title'].isna().sum() > 0:\n",
    "    books_df['title'] = books_df['title'].fillna('Unknown Title')\n",
    "books_df['title'] = books_df['title'].astype(str).str.strip()\n",
    "books_df.loc[books_df['title'] == '', 'title'] = 'Unknown Title'\n",
    "\n",
    "# authors - –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏\n",
    "if books_df['authors'].isna().sum() > 0:\n",
    "    books_df['authors'] = books_df['authors'].fillna('Unknown Author')\n",
    "books_df['authors'] = books_df['authors'].astype(str).str.strip()\n",
    "books_df.loc[books_df['authors'] == '', 'authors'] = 'Unknown Author'\n",
    "\n",
    "# language_code - –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ + –ö–û–î–ò–†–£–ï–ú (–ò–°–ü–†–ê–í–õ–ï–ù–û!)\n",
    "books_df['language_code'] = books_df['language_code'].fillna('eng').astype(str).str.strip()\n",
    "books_df.loc[books_df['language_code'] == '', 'language_code'] = 'eng'\n",
    "\n",
    "# –ö–æ–¥–∏—Ä—É–µ–º language_code –≤ —á–∏—Å–ª–∞\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_lang = LabelEncoder()\n",
    "books_df['language_code_encoded'] = le_lang.fit_transform(books_df['language_code'])\n",
    "\n",
    "print(f\"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: title, authors, language_code\")\n",
    "print(f\"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤: {books_df['language_code'].nunique()}\")\n",
    "print(f\"   language_code_encoded: –¥–∏–∞–ø–∞–∑–æ–Ω 0-{books_df['language_code_encoded'].max()}\")\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# 4. –°–û–ó–î–ê–ù–ò–ï –ú–ê–ü–ü–ò–ù–ì–ê ID\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n[5/9] –°–æ–∑–¥–∞–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ ID...\")\n",
    "\n",
    "# books_df['id'] = –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π ID (1..10000) ‚Üê –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ ratings\n",
    "# books_df['book_id'] = —Ä–µ–∞–ª—å–Ω—ã–π Goodreads ID ‚Üê –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ book_tags\n",
    "\n",
    "internal_to_goodreads = dict(zip(books_df['id'], books_df['book_id']))\n",
    "goodreads_to_internal = dict(zip(books_df['book_id'], books_df['id']))\n",
    "\n",
    "print(f\"   –ú–∞–ø–ø–∏–Ω–≥ —Å–æ–∑–¥–∞–Ω: {len(internal_to_goodreads):,} –∫–Ω–∏–≥\")\n",
    "\n",
    "# ================================================================================\n",
    "# 5. –û–ë–†–ê–ë–û–¢–ö–ê –¢–ï–ì–û–í (–ë–ï–ó –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø –¢–û–ü-20)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n[6/9] –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–≥–æ–≤...\")\n",
    "\n",
    "# –§–∏–ª—å—Ç—Ä—É–µ–º –º—É—Å–æ—Ä–Ω—ã–µ —Ç–µ–≥–∏\n",
    "def is_valid_tag(tag):\n",
    "    tag = str(tag).lower().strip()\n",
    "    if len(tag) < 2 or len(tag) > 50:\n",
    "        return False\n",
    "    if tag.startswith(('-', '.', '#', '@')):\n",
    "        return False\n",
    "    # –£–¥–∞–ª—è–µ–º —Ç–µ–≥–∏ —Å —é–Ω–∏–∫–æ–¥-—Å–∏–º–≤–æ–ª–∞–º–∏\n",
    "    if not all(ord(c) < 128 for c in tag):\n",
    "        return False\n",
    "    # –£–¥–∞–ª—è–µ–º —á–∏—Å—Ç–æ —Ü–∏—Ñ—Ä–æ–≤—ã–µ —Ç–µ–≥–∏\n",
    "    if tag.replace('-', '').isdigit():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "tags_df_clean = tags_df[tags_df['tag_name'].apply(is_valid_tag)].copy()\n",
    "print(f\"   –ß–∏—Å—Ç—ã—Ö —Ç–µ–≥–æ–≤: {len(tags_df_clean):,} / {len(tags_df):,}\")\n",
    "\n",
    "# –ú–µ—Ä–∂–∏–º book_tags —Å —á–∏—Å—Ç—ã–º–∏ —Ç–µ–≥–∞–º–∏\n",
    "book_tags_clean = book_tags_df.merge(tags_df_clean, on='tag_id', how='inner')\n",
    "print(f\"   –ó–∞–ø–∏—Å–µ–π book_tags –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(book_tags_clean):,}\")\n",
    "\n",
    "# –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –í–°–ï —Ç–µ–≥–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–Ω–∏–≥–∏ (—Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ count, —á—Ç–æ–±—ã —Å–∞–º—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –±—ã–ª–∏ –ø–µ—Ä–≤—ã–º–∏)\n",
    "book_tags_sorted = book_tags_clean.sort_values(['goodreads_book_id', 'count'], ascending=[True, False])\n",
    "\n",
    "book_tags_aggregated = (\n",
    "    book_tags_sorted\n",
    "    .groupby('goodreads_book_id')['tag_name']\n",
    "    .apply(lambda x: ' '.join(x.astype(str)))\n",
    "    .reset_index()\n",
    ")\n",
    "book_tags_aggregated.columns = ['goodreads_book_id', 'tags_text']\n",
    "\n",
    "print(f\"  –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–æ –í–°–ï–• —Ç–µ–≥–æ–≤ –¥–ª—è {len(book_tags_aggregated):,} –∫–Ω–∏–≥ (–ë–ï–ó –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç–æ–ø-20)\")\n",
    "\n",
    "# –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–≥–æ–≤ –Ω–∞ –∫–Ω–∏–≥—É\n",
    "avg_tags = book_tags_sorted.groupby('goodreads_book_id').size().mean()\n",
    "print(f\"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–≥–æ–≤ –Ω–∞ –∫–Ω–∏–≥—É: {avg_tags:.1f}\")\n",
    "\n",
    "# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Goodreads ID ‚Üí internal ID\n",
    "book_tags_aggregated['internal_id'] = book_tags_aggregated['goodreads_book_id'].map(goodreads_to_internal)\n",
    "book_tags_aggregated = book_tags_aggregated.dropna(subset=['internal_id'])\n",
    "book_tags_aggregated['internal_id'] = book_tags_aggregated['internal_id'].astype(int)\n",
    "\n",
    "print(f\"   –ü–æ—Å–ª–µ –º–∞–ø–ø–∏–Ω–≥–∞: {len(book_tags_aggregated):,} –∫–Ω–∏–≥\")\n",
    "\n",
    "# ================================================================================\n",
    "# 6. –ú–ï–†–î–ñ –í –ï–î–ò–ù–´–ô –î–ê–¢–ê–°–ï–¢ (–î–û SPLIT)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n[7/9] –°–æ–∑–¥–∞–Ω–∏–µ –µ–¥–∏–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...\")\n",
    "\n",
    "complete_df = ratings_df.copy()\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥\n",
    "books_metadata = books_df[[\n",
    "    'id', 'title', 'authors', 'average_rating', 'language_code_encoded',\n",
    "    'year_normalized', 'publication_era', 'ratings_count', 'work_ratings_count'\n",
    "]].rename(columns={'id': 'book_id'})\n",
    "\n",
    "before_merge = len(complete_df)\n",
    "complete_df = complete_df.merge(books_metadata, on='book_id', how='left')\n",
    "lost_rows = before_merge - len(complete_df)\n",
    "print(f\"   –ú–µ—Ä–¥–∂ —Å books: {before_merge:,} ‚Üí {len(complete_df):,} (–ø–æ—Ç–µ—Ä—è–Ω–æ: {lost_rows})\")\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥–∏\n",
    "book_tags_for_merge = book_tags_aggregated[['internal_id', 'tags_text']].rename(columns={'internal_id': 'book_id'})\n",
    "before_merge = len(complete_df)\n",
    "complete_df = complete_df.merge(book_tags_for_merge, on='book_id', how='left')\n",
    "lost_rows = before_merge - len(complete_df)\n",
    "print(f\"   –ú–µ—Ä–¥–∂ —Å tags: {before_merge:,} ‚Üí {len(complete_df):,} (–ø–æ—Ç–µ—Ä—è–Ω–æ: {lost_rows})\")\n",
    "\n",
    "# –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ —Ç–µ–≥–æ–≤\n",
    "complete_df['tags_text'] = complete_df['tags_text'].fillna('')\n",
    "\n",
    "print(f\"   Complete dataset: {len(complete_df):,} —Å—Ç—Ä–æ–∫ √ó {len(complete_df.columns)} –∫–æ–ª–æ–Ω–æ–∫\")\n",
    "print(f\"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–Ω–∏–≥: {complete_df['book_id'].nunique():,}\")\n",
    "print(f\"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {complete_df['user_id'].nunique():,}\")\n",
    "print(f\"   –ö–Ω–∏–≥ —Å —Ç–µ–≥–∞–º–∏: {(complete_df['tags_text'] != '').sum():,}\")\n",
    "\n",
    "# ================================================================================\n",
    "# # ================================================================================\n",
    "# 7. TRAIN/TEST SPLIT (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê + –û–ë–†–ê–ë–û–¢–ö–ê –î–£–ë–õ–ò–ö–ê–¢–û–í)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n[8/9] Train/Test split (temporal + stratified)...\")\n",
    "\n",
    "def stratified_temporal_split(df, test_size=3, min_interactions=5):\n",
    "    \"\"\"\n",
    "    –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å ‚â•5 –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è–º–∏:\n",
    "    - –ü–æ—Å–ª–µ–¥–Ω–∏–µ test_size –£–ù–ò–ö–ê–õ–¨–ù–´–• –ö–ù–ò–ì ‚Üí test\n",
    "    - –û—Å—Ç–∞–ª—å–Ω–æ–µ ‚Üí train\n",
    "    \n",
    "    –î–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å <5 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–Ω–∏–≥:\n",
    "    - –í—Å—ë ‚Üí train\n",
    "    \n",
    "    –ï—Å–ª–∏ —é–∑–µ—Ä –æ—Ü–µ–Ω–∏–ª –∫–Ω–∏–≥—É –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ - –≤—Å–µ –æ—Ü–µ–Ω–∫–∏ –∏–¥—É—Ç –≤ –æ–¥–∏–Ω —Å–ø–ª–∏—Ç\n",
    "    \"\"\"\n",
    "    train_list, test_list = [], []\n",
    "    holdout_dict = {}\n",
    "    \n",
    "    stats = {\n",
    "        'users_total': 0,\n",
    "        'users_in_test': 0,\n",
    "        'users_only_train': 0,\n",
    "        'train_size': 0,\n",
    "        'test_size': 0,\n",
    "        'duplicates_handled': 0\n",
    "    }\n",
    "    \n",
    "    for user_id, group in df.groupby('user_id'):\n",
    "        stats['users_total'] += 1\n",
    "        \n",
    "        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ book_id (—á—Ç–æ–±—ã –≤—Å–µ –æ—Ü–µ–Ω–∫–∏ –æ–¥–Ω–æ–π –∫–Ω–∏–≥–∏ —à–ª–∏ –≤–º–µ—Å—Ç–µ)\n",
    "        book_groups = []\n",
    "        for book_id, book_ratings in group.groupby('book_id'):\n",
    "            if len(book_ratings) > 1:\n",
    "                stats['duplicates_handled'] += len(book_ratings) - 1\n",
    "            book_groups.append((book_id, book_ratings))\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –£–ù–ò–ö–ê–õ–¨–ù–´–• –∫–Ω–∏–≥\n",
    "        n_unique_books = len(book_groups)\n",
    "        \n",
    "        if n_unique_books < min_interactions:\n",
    "            # –ú–∞–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–Ω–∏–≥ ‚Üí –≤—Å—ë –≤ train\n",
    "            train_list.append(group)\n",
    "            stats['train_size'] += len(group)\n",
    "            stats['users_only_train'] += 1\n",
    "            continue\n",
    "        \n",
    "        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ test_size –ö–ù–ò–ì –≤ test\n",
    "        test_books = book_groups[-test_size:]\n",
    "        train_books = book_groups[:-test_size]\n",
    "        \n",
    "        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—ã\n",
    "        test_group = pd.concat([ratings for _, ratings in test_books], ignore_index=False)\n",
    "        train_group = pd.concat([ratings for _, ratings in train_books], ignore_index=False)\n",
    "        \n",
    "        train_list.append(train_group)\n",
    "        test_list.append(test_group)\n",
    "        \n",
    "        # –í holdout —Å–æ—Ö—Ä–∞–Ω—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ book_id\n",
    "        holdout_dict[user_id] = [book_id for book_id, _ in test_books]\n",
    "        \n",
    "        stats['users_in_test'] += 1\n",
    "        stats['train_size'] += len(train_group)\n",
    "        stats['test_size'] += len(test_group)\n",
    "    \n",
    "    train_df = pd.concat(train_list, ignore_index=True)\n",
    "    test_df = pd.concat(test_list, ignore_index=True) if test_list else pd.DataFrame()\n",
    "    \n",
    "    return train_df, test_df, holdout_dict, stats\n",
    "\n",
    "train_df, test_df, holdout_dict, stats = stratified_temporal_split(complete_df)\n",
    "\n",
    "print(f\"   Train: {len(train_df):,} –æ—Ü–µ–Ω–æ–∫\")\n",
    "print(f\"   Test: {len(test_df):,} –æ—Ü–µ–Ω–æ–∫\")\n",
    "print(f\"   –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ test: {stats['users_in_test']:,} / {stats['users_total']:,}\")\n",
    "print(f\"    –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Ç–æ–ª—å–∫–æ –≤ train (< 5 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–Ω–∏–≥): {stats['users_only_train']:,}\")\n",
    "print(f\"    –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['duplicates_handled']:,}\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ data leakage\n",
    "train_pairs = set(zip(train_df['user_id'], train_df['book_id']))\n",
    "test_pairs = set(zip(test_df['user_id'], test_df['book_id']))\n",
    "leakage = train_pairs & test_pairs\n",
    "\n",
    "if len(leakage) > 0:\n",
    "    print(f\"   Data leakage: {len(leakage)} –ø–∞—Ä (user_id, book_id) –≤ –æ–±–æ–∏—Ö —Å–µ—Ç–∞—Ö!\")\n",
    "    print(f\"     –ü—Ä–∏–º–µ—Ä—ã: {list(leakage)[:5]}\")\n",
    "    raise ValueError(\"Data leakage detected!\")\n",
    "else:\n",
    "    print(f\"   Data leakage check: OK (–Ω–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –ø–æ (user_id, book_id))\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ–π—Ç–∏–Ω–≥–æ–≤\n",
    "print(f\"    –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤:\")\n",
    "print(f\"     Train: {train_df['rating'].value_counts().sort_index().to_dict()}\")\n",
    "print(f\"     Test:  {test_df['rating'].value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 8. –°–û–ó–î–ê–ù–ò–ï EMBEDDINGS \n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n[9/10] –°–æ–∑–¥–∞–Ω–∏–µ embeddings...\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"  üñ•Ô∏è  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device.upper()}\")\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "print(f\"   –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (dim=384)\")\n",
    "\n",
    "# ============================================================================\n",
    "# A) BOOK TAG EMBEDDINGS \n",
    "# ============================================================================\n",
    "\n",
    "all_books = complete_df['book_id'].unique()\n",
    "book_tags_available = book_tags_aggregated[book_tags_aggregated['internal_id'].isin(all_books)]\n",
    "\n",
    "book_tags_list = book_tags_available['tags_text'].tolist()\n",
    "book_ids_list = book_tags_available['internal_id'].tolist()\n",
    "\n",
    "print(f\"   –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–≥–æ–≤ –¥–ª—è {len(book_tags_list):,} –∫–Ω–∏–≥...\")\n",
    "book_tag_embeddings_array = model.encode(book_tags_list, batch_size=256, show_progress_bar=True)\n",
    "\n",
    "book_tag_embeddings = {book_ids_list[i]: book_tag_embeddings_array[i] for i in range(len(book_ids_list))}\n",
    "\n",
    "# –î–ª—è –∫–Ω–∏–≥ –±–µ–∑ —Ç–µ–≥–æ–≤ - –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä\n",
    "for book_id in all_books:\n",
    "    if book_id not in book_tag_embeddings:\n",
    "        book_tag_embeddings[book_id] = np.zeros(384)\n",
    "\n",
    "print(f\"   Book tag embeddings: {len(book_tag_embeddings):,} √ó 384\")\n",
    "\n",
    "# ============================================================================\n",
    "# B) BOOK TEXT EMBEDDINGS (title + authors) \n",
    "# ============================================================================\n",
    "\n",
    "print(f\"   –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ title + authors –¥–ª—è {len(books_df):,} –∫–Ω–∏–≥...\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ: \"title by authors\"\n",
    "books_df['text_for_embedding'] = books_df['title'] + ' by ' + books_df['authors']\n",
    "book_text_list = books_df['text_for_embedding'].tolist()\n",
    "book_ids_for_text = books_df['id'].tolist()  # internal ID\n",
    "\n",
    "print(f\"   –ü—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞:\")\n",
    "print(f\"     ‚Ä¢ {book_text_list[0][:80]}...\")\n",
    "print(f\"     ‚Ä¢ {book_text_list[100][:80]}...\")\n",
    "\n",
    "book_text_embeddings_array = model.encode(book_text_list, batch_size=256, show_progress_bar=True)\n",
    "\n",
    "book_text_embeddings = {book_ids_for_text[i]: book_text_embeddings_array[i] for i in range(len(book_ids_for_text))}\n",
    "\n",
    "print(f\"   Book text embeddings: {len(book_text_embeddings):,} √ó 384\")\n",
    "\n",
    "# ============================================================================\n",
    "# C) –ö–û–ú–ë–ò–ù–ò–†–û–í–ê–ù–ù–´–ï BOOK EMBEDDINGS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"   –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö book embeddings...\")\n",
    "\n",
    "# –î–ª—è –∫–∞–∂–¥–æ–π –∫–Ω–∏–≥–∏: [tag_emb (384) + text_emb (384)] = 768\n",
    "book_embeddings_combined = {}\n",
    "\n",
    "for book_id in all_books:\n",
    "    tag_emb = book_tag_embeddings.get(book_id, np.zeros(384))\n",
    "    text_emb = book_text_embeddings.get(book_id, np.zeros(384))\n",
    "    book_embeddings_combined[book_id] = np.concatenate([tag_emb, text_emb])\n",
    "\n",
    "print(f\"   Combined book embeddings: {len(book_embeddings_combined):,} √ó 768\")\n",
    "\n",
    "# ============================================================================\n",
    "# D) USER EMBEDDINGS (–Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö book embeddings)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"   User embeddings...\")\n",
    "\n",
    "def create_user_embeddings(df, book_emb_dict):\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–µ—Ç user embeddings –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–≥–æ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è book embeddings\n",
    "    \"\"\"\n",
    "    user_history = df.groupby('user_id', group_keys=False).apply(\n",
    "        lambda x: list(zip(x['book_id'], x['rating'])), include_groups=False\n",
    "    ).to_dict()\n",
    "    \n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "    emb_dim = len(next(iter(book_emb_dict.values())))\n",
    "    \n",
    "    user_emb = {}\n",
    "    for i, (user_id, books_ratings) in enumerate(user_history.items()):\n",
    "        weighted_sum = np.zeros(emb_dim)\n",
    "        weight_sum = 0\n",
    "        \n",
    "        for book_id, rating in books_ratings:\n",
    "            if book_id in book_emb_dict:\n",
    "                weight = (rating - 1) / 4.0  # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è [0, 1]\n",
    "                weighted_sum += weight * book_emb_dict[book_id]\n",
    "                weight_sum += weight\n",
    "        \n",
    "        user_emb[user_id] = weighted_sum / weight_sum if weight_sum > 0 else np.zeros(emb_dim)\n",
    "        \n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f\"     {i + 1}/{len(user_history):,}...\")\n",
    "    \n",
    "    return user_emb\n",
    "\n",
    "print(f\"   Train user embeddings...\")\n",
    "train_user_embeddings = create_user_embeddings(train_df, book_embeddings_combined)\n",
    "print(f\"   Train user embeddings: {len(train_user_embeddings):,} √ó 768\")\n",
    "\n",
    "print(f\"   Test user embeddings...\")\n",
    "test_user_embeddings = create_user_embeddings(test_df, book_embeddings_combined)\n",
    "print(f\"   Test user embeddings: {len(test_user_embeddings):,} √ó 768\")\n",
    "\n",
    "# ================================================================================\n",
    "# 9. –§–ò–ù–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–¢–ê–°–ï–¢–û–í\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n[10/10] –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ...\")\n",
    "\n",
    "# –î—Ä–æ–ø–∞–µ–º tags_text –∏–∑ train/test (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≤ complete)\n",
    "train_final = train_df.drop(columns=['tags_text'])\n",
    "test_final = test_df.drop(columns=['tags_text'])\n",
    "complete_final = complete_df.copy()\n",
    "\n",
    "print(f\"   tags_text —É–¥–∞–ª–µ–Ω –∏–∑ train/test (–æ—Å—Ç–∞–≤–ª–µ–Ω –≤ complete)\")\n",
    "print(f\"   Train final: {train_final.shape}\")\n",
    "print(f\"   Test final: {test_final.shape}\")\n",
    "print(f\"   Complete: {complete_final.shape}\")\n",
    "\n",
    "# ================================================================================\n",
    "# –°–û–•–†–ê–ù–ï–ù–ò–ï\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...\")\n",
    "\n",
    "# CSV\n",
    "print(\"  [1/8] complete_dataset.csv...\")\n",
    "complete_final.to_csv('complete_dataset.csv', index=False)\n",
    "\n",
    "print(\"  [2/8] train_dataset.csv...\")\n",
    "train_final.to_csv('train_dataset.csv', index=False)\n",
    "\n",
    "print(\"  [3/8] test_dataset.csv...\")\n",
    "test_final.to_csv('test_dataset.csv', index=False)\n",
    "\n",
    "# PKL - embeddings \n",
    "print(\"  [4/8] train_embeddings.pkl...\")\n",
    "train_embeddings = {\n",
    "    'book_emb': np.vstack([book_embeddings_combined[bid] for bid in train_final['book_id']]),\n",
    "    'user_emb': np.vstack([train_user_embeddings[uid] for uid in train_final['user_id']]),\n",
    "    'book_ids': train_final['book_id'].values,\n",
    "    'user_ids': train_final['user_id'].values,\n",
    "    'language_code': train_final['language_code_encoded'].values  \n",
    "}\n",
    "pickle.dump(train_embeddings, open('train_embeddings.pkl', 'wb'), protocol=4)\n",
    "\n",
    "print(\"  [5/8] test_embeddings.pkl...\")\n",
    "test_embeddings = {\n",
    "    'book_emb': np.vstack([book_embeddings_combined[bid] for bid in test_final['book_id']]),\n",
    "    'user_emb': np.vstack([test_user_embeddings[uid] for uid in test_final['user_id']]),\n",
    "    'book_ids': test_final['book_id'].values,\n",
    "    'user_ids': test_final['user_id'].values,\n",
    "    'language_code': test_final['language_code_encoded'].values  # –ù–û–í–û–ï!\n",
    "}\n",
    "pickle.dump(test_embeddings, open('test_embeddings.pkl', 'wb'), protocol=4)\n",
    "\n",
    "print(\"  [6/8] –°–ª–æ–≤–∞—Ä–∏ embeddings...\")\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –í–°–ï —Ç–∏–ø—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –≥–∏–±–∫–æ—Å—Ç–∏\n",
    "embeddings_dict = {\n",
    "    'book_combined': book_embeddings_combined,      # 768-dim (tags + text)\n",
    "    'book_tags': book_tag_embeddings,               # 384-dim (—Ç–æ–ª—å–∫–æ —Ç–µ–≥–∏)\n",
    "    'book_text': book_text_embeddings               # 384-dim (—Ç–æ–ª—å–∫–æ title+authors)\n",
    "}\n",
    "pickle.dump(embeddings_dict, open('book_embeddings.pkl', 'wb'), protocol=4)\n",
    "\n",
    "pickle.dump(\n",
    "    {'train': train_user_embeddings, 'test': test_user_embeddings},\n",
    "    open('user_embeddings.pkl', 'wb'),\n",
    "    protocol=4\n",
    ")\n",
    "\n",
    "print(\"  [7/8] holdout_dict.pkl...\")\n",
    "pickle.dump(holdout_dict, open('holdout_dict.pkl', 'wb'), protocol=4)\n",
    "\n",
    "# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n",
    "print(\"  [8/8] preprocessing_metadata.pkl...\")\n",
    "metadata = {\n",
    "    'n_ratings_total': len(complete_final),\n",
    "    'n_ratings_train': len(train_final),\n",
    "    'n_ratings_test': len(test_final),\n",
    "    'n_users_total': complete_final['user_id'].nunique(),\n",
    "    'n_users_train': len(train_user_embeddings),\n",
    "    'n_users_test': len(test_user_embeddings),\n",
    "    'n_books': len(book_embeddings_combined),\n",
    "    'embedding_dims': {\n",
    "        'book_tags': 384,\n",
    "        'book_text': 384,\n",
    "        'book_combined': 768,\n",
    "        'user': 768,\n",
    "        'language_code': 1\n",
    "    },\n",
    "    'publication_era_mapping': {\n",
    "        0: 'Unknown',\n",
    "        1: 'Ancient',\n",
    "        2: 'Medieval',\n",
    "        3: 'Early Modern',\n",
    "        4: '19th century',\n",
    "        5: 'Early 20th (1900-1950)',\n",
    "        6: 'Late 20th (1950-2000)',\n",
    "        7: '21st century'\n",
    "    },\n",
    "    'rating_distribution_train': train_final['rating'].value_counts().to_dict(),\n",
    "    'rating_distribution_test': test_final['rating'].value_counts().to_dict(),\n",
    "    'duplicates_kept': duplicates,\n",
    "    'n_languages': books_df['language_code'].nunique()\n",
    "}\n",
    "pickle.dump(metadata, open('preprocessing_metadata.pkl', 'wb'), protocol=4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING –ó–ê–í–ï–†–®–ï–ù\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n–°–æ–∑–¥–∞–Ω–æ 8 —Ñ–∞–π–ª–æ–≤:\")\n",
    "print(f\"  CSV:\")\n",
    "print(f\"    ‚Ä¢ complete_dataset.csv: {complete_final.shape[0]:,} √ó {complete_final.shape[1]} (—Å tags_text)\")\n",
    "print(f\"    ‚Ä¢ train_dataset.csv: {train_final.shape[0]:,} √ó {train_final.shape[1]} (–±–µ–∑ tags_text)\")\n",
    "print(f\"    ‚Ä¢ test_dataset.csv: {test_final.shape[0]:,} √ó {test_final.shape[1]} (–±–µ–∑ tags_text)\")\n",
    "print(f\"  PKL:\")\n",
    "print(f\"    ‚Ä¢ train_embeddings.pkl: book {train_embeddings['book_emb'].shape} + user {train_embeddings['user_emb'].shape} + language\")\n",
    "print(f\"    ‚Ä¢ test_embeddings.pkl: book {test_embeddings['book_emb'].shape} + user {test_embeddings['user_emb'].shape} + language\")\n",
    "print(f\"    ‚Ä¢ book_embeddings.pkl: 3 —Ç–∏–ø–∞ (combined 768, tags 384, text 384)\")\n",
    "print(f\"    ‚Ä¢ user_embeddings.pkl: train {len(train_user_embeddings):,} + test {len(test_user_embeddings):,} (768-dim)\")\n",
    "print(f\"    ‚Ä¢ holdout_dict.pkl: {len(holdout_dict):,} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\")\n",
    "print(f\"    ‚Ä¢ preprocessing_metadata.pkl\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl_env_cuda)",
   "language": "python",
   "name": "dl_env_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
